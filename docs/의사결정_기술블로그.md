## 1. 서론

### 1.1 프로젝트 배경

이 프로젝트는 러스트로 개발한 거래소 체결 엔진을 자바로 마이그레이션하면서,  
**JVM 기반 시스템의 성능 특성과 한계를 직접 검증하기 위해** 시작되었다.

기존에는 NestJS(Node.js) 기반 백엔드 개발을 주력으로 해왔고,
Java와 Spring을 사용한 경험은 있었지만  
JVM의 메모리 관리와 GC 동작을 성능 의사결정의 관점에서 깊이 다뤄본 적은 없었다.

최근 자바 백엔드 채용 과정에서  
JVM 튜닝과 GC에 대한 질문을 반복적으로 접하며,  
옵션을 외우는 방식이 아니라  
**“JVM이 무엇을 자동으로 처리하고,  
어디서부터 개발자가 책임져야 하는지”를  
실제 시스템을 통해 이해할 필요성**을 느꼈다.

그래서 이전에 개발하여 실제 서비스 중인
러스트 기반 거래소 체결 엔진의 코어 로직을 기준점으로 삼아,
동일한 기능을 자바로 구현하고
JVM 튜닝이 성능에 미치는 영향을 비교·분석하기로 했다.
동일한 로직을 자바로 구현하고  
JVM 튜닝이 실제 성능에 어떤 영향을 미치는지를  
벤치마크와 실험을 통해 확인하기로 했다.

---

### 1.2 거래소 체결 엔진이란?

거래소 체결 엔진은 사용자가 제출한 주문을 받아  
정해진 우선순위 규칙에 따라 매칭하고,  
체결 결과를 결정하는 시스템의 핵심 컴포넌트다.

**주요 기능은 다음과 같다.**

- 주문 수신 및 유효성 검증
- Price-Time Priority 기반 매칭  
  (동일 가격에서는 먼저 제출된 주문 우선)
- 체결 실행 및 잔고 업데이트
- 호가창(Order Book) 관리

이 컴포넌트는 거래소 전체 시스템에서  
가장 높은 처리량과 가장 낮은 지연시간을 동시에 요구받는다.

**핵심 성능 요구사항은 다음과 같다.**

- **낮은 지연시간**: 주문 제출부터 체결까지의 응답 지연 최소화
- **높은 처리량**: 초당 수만 건 이상의 주문 처리
- **일관된 성능**: GC pause 등으로 인한 지연 변동 최소화

이러한 특성 때문에  
GC 동작, 메모리 관리 방식, 객체 생명주기는  
체결 엔진의 성능에 직접적인 영향을 미친다.

---

### 1.3 이 글의 관점과 핵심 질문

이 글은 JVM 튜닝 옵션을 나열하거나  
특정 설정을 정답처럼 제시하는 것을 목표로 하지 않는다.

대신 다음과 같은 질문에 답하는 과정을 기록한다.

- JVM 튜닝에서 실제로 가장 큰 효과를 낸 변경은 무엇이었는가?
- GC 교체(ZGC)는 언제 효과적이고, 언제 오히려 성능을 악화시키는가?
- JVM이 자동으로 해주는 영역과 개발자가 통제해야 할 영역은 어디까지인가?
- 측정 결과가 예상과 달랐을 때, 어떤 의사결정을 내려야 하는가?

이 글은  
**가설 수립 → 실행 → 측정 → 검증 → 폐기 또는 채택**의 과정을 통해  
JVM 튜닝이 실제 시스템 성능에 미치는 영향을  
있는 그대로 정리한 실험 기록이다.

---

## 2. 프로젝트 개요

### 2.1 거래소 체결 엔진 구조

본 프로젝트에서 사용한 체결 엔진은  
JVM 및 코드 레벨 성능 특성을 명확히 관측하기 위해  
의도적으로 단순화된 구조를 가진다.

**주요 컴포넌트는 다음과 같다.**

- **OrderBook**: TreeMap 기반 호가창  
  (가격 우선, 동일 가격에서는 시간 우선)
- **Matcher**: 주문 매칭 로직  
  (FIFO 기반 체결)
- **Executor**: 체결 실행 및 잔고 업데이트
- **BalanceCache**: 메모리 기반 잔고 관리

**주문 처리 흐름은 다음과 같다.**

```
주문 제출 → 잔고 잠금 → OrderBook 등록 → 매칭 시도 → 체결 실행 → 결과 반환
```

**설계 상의 주요 특징은 다음과 같다.**

- **싱글 스레드 동기 처리**
  - 멀티스레드 경합, 락 비용, 스케줄링 영향을 제거
  - GC 및 객체 생성 비용을 더 명확히 관측하기 위함
- **메모리 기반 처리**
  - DB, WAL, 디스크 I/O를 제외
  - 순수 JVM 메모리 관리와 GC 특성에 집중
- **직접 호출 구조**
  - 큐, 채널, 네트워크 레이어 제거
  - 불필요한 오버헤드를 최소화한 상태에서 성능 측정

이 구조는 실서비스 아키텍처를 재현하기 위한 것이 아니라,  
**JVM 튜닝과 코드 변경의 효과를 왜곡 없이 관측하기 위한  
실험용 기준 구조**로 설계되었다.

---

### 2.2 러스트 → 자바 마이그레이션 접근 방식

기준점으로 사용한 러스트 버전 체결 엔진은  
이전에 개발하여 실제 서비스 환경에서 운영 중인 코드로,  
이미 성능 특성과 병목 지점이 명확히 파악된 상태였다.

**러스트 구현의 주요 특성은 다음과 같다.**

- Zero-cost abstraction을 통한 높은 처리 성능
- 컴파일 타임 메모리 안전성 보장
- GC 없이 명시적인 메모리 생명주기 관리

자바로의 마이그레이션에서는  
기능 확장이나 구조 변경을 최소화하고,  
러스트 버전과 **동일한 로직과 처리 흐름을 유지하는 것**을  
가장 중요한 원칙으로 삼았다.

이를 통해 다음과 같은 차이를 명확히 관측하고자 했다.

- GC 도입으로 인한 성능 변동성
- 객체 할당 및 생명주기 관리 비용
- JIT 컴파일이 반복 실행 코드에 미치는 영향

즉, 이 마이그레이션은  
“자바로 더 좋은 설계를 만들기 위한 작업”이 아니라,  
**동일한 문제를 서로 다른 런타임에서 실행했을 때  
어떤 비용과 이득이 발생하는지를 비교하기 위한 실험**이었다.

---

## 3. 환경 및 벤치마크 설정

### 3.1 하드웨어 환경

벤치마크는 개인 개발 환경에서 수행되었으며,  
다음과 같은 하드웨어 사양을 기준으로 한다.

- CPU: Apple M3 (11-core)
- Memory: 18GB
- OS: macOS (ARM64)

Apple Silicon(ARM64) 환경이라는 점에서  
일반적인 x86 서버 환경과는 네이티브 코드 생성 및 캐시 특성에 차이가 있다.  
따라서 본 문서에서는 절대적인 성능 수치보다는  
**동일 환경 내에서 설정 변경 전후의 상대적인 변화**를 중심으로 결과를 해석한다.

---

### 3.2 소프트웨어 환경

벤치마크에 사용한 소프트웨어 환경은 다음과 같다.

- JDK: OpenJDK 17.0.11 (Amazon Corretto)
- Spring Boot: 4.0.1
- Build Tool: Gradle
- Benchmark Tool: JMH (Java Microbenchmark Harness) 1.37

Java 17은 장기 지원(LTS) 버전으로,  
실제 운영 환경과의 차이를 최소화하기 위해 선택했다.

---

### 3.3 벤치마크 설계 원칙

성능 측정에는 JMH를 사용했다.  
이는 JVM의 JIT 컴파일, 인라이닝, 데드 코드 제거 등으로 인한  
측정 결과 왜곡을 최소화하기 위한 선택이다.

본 벤치마크의 목적은 최대 성능을 측정하는 것이 아니라,  
**설정 변경에 따른 성능 차이를 일관되게 비교하는 것**이다.  
이를 위해 다음과 같은 원칙을 적용했다.

- 워밍업(Warmup)과 측정을 명확히 분리
- 단일 JVM 인스턴스에서 반복 측정
- 설정 변경 간 비교 가능성을 최우선으로 고려

워밍업과 측정은 각각 3회씩 수행했으며,  
각 회차는 2초 동안 실행된다.  
이는 JIT 최적화가 안정화된 이후의 성능을  
재현 가능하게 관측하기 위한 최소한의 설정이다.

실제로 워밍업 횟수를 1회에서 3회로 늘렸을 때,  
측정 결과의 분산이 크게 줄어들었고  
JVM 옵션 변경에 따른 성능 차이가 보다 명확하게 드러났다.

---

### 3.4 측정 지표와 테스트 시나리오

주요 측정 지표는 다음과 같다.

- TPS (Transactions Per Second): 초당 처리 가능한 주문 수
- GC Pause Time: GC로 인한 애플리케이션 중단 시간
- 측정 단위: μs/op (operation당 소요 시간)

테스트 시나리오는 거래소 체결 엔진의 주요 사용 패턴을 기준으로 구성했다.

- limitOrderTps: 지정가 주문 처리 성능
- marketBuyTps: 시장가 매수 주문 처리 성능
- mixedTps: 지정가 매수·매도와 시장가 주문을 혼합한 시나리오

주문 수는 1,000 / 5,000 / 10,000 / 50,000 단위로 조정하여,  
부하 증가에 따른 성능 변화와 JVM 동작 특성을 관찰했다.

---

### 3.5 초기 상태 설정

벤치마크 결과의 재현성을 확보하기 위해  
모든 테스트는 동일한 초기 상태에서 시작하도록 설정했다.

- 사용자 수: 100명
- 초기 잔고: SOL 10,000 / USDT 10,000,000
- 시드 오더북: 100.00 USDT에 1,000,000 SOL 매도 호가

이를 통해 잔고 부족이나 예외 상황이  
성능 측정에 영향을 주지 않도록 통제했다.

---

## 4. JVM과 GC: 실험을 이해하기 위한 최소한의 배경

본 장은 JVM 내부 구조를 포괄적으로 설명하는 것을 목표로 하지 않는다.  
이후의 튜닝 결과를 해석하기 위해 **반드시 필요한 지점만**을 정리한다.

이 프로젝트에서 중요한 질문은 다음과 같다.

- JVM은 객체의 생명주기를 어떻게 관리하는가?
- 이 관리 방식이 TPS와 지연시간에 어떤 영향을 주는가?
- 개발자가 통제할 수 있는 영역과, JVM에 맡겨야 하는 영역은 어디까지인가?

---

### 4.1 JVM의 역할: 런타임이 개입하는 지점

JVM은 단순히 바이트코드를 실행하는 런타임이 아니다.  
객체의 생성, 이동, 소멸 시점을 **런타임에서 결정**하며,  
이 과정에 GC와 JIT 컴파일러가 깊게 개입한다.

Rust와 비교하면 이 차이가 명확해진다.

- Rust: 객체의 생명주기가 컴파일 타임에 결정된다.
- JVM: 객체의 생존 여부와 이동 시점이 런타임에 결정된다.

즉, JVM 기반 시스템에서는  
코드가 동일하더라도 **실행 시점의 메모리 상태와 GC 동작에 따라  
성능 특성이 달라질 수 있다.**

이 실험은 바로 이 “런타임 개입 비용”이  
실제 TPS와 지연시간에 어떤 형태로 나타나는지를 관측하는 데 목적이 있다.

---

### 4.2 힙 메모리와 객체 생명주기

JVM에서 대부분의 객체는 힙(Heap)에 할당된다.  
G1GC 기준으로 힙은 여러 Region으로 나뉘며,  
객체는 생존 시간에 따라 Young 영역과 Old 영역을 이동한다.

이 구조에서 중요한 점은 다음 두 가지다.

- 대부분의 객체는 매우 짧은 생명주기를 가진다.
- 오래 살아남는 객체는 GC 비용에 직접적인 영향을 준다.

거래소 체결 엔진의 경우,

- 주문 객체, 매칭 중간 객체는 매우 빠르게 생성·소멸되고
- 호가창, 잔고 정보는 상대적으로 오래 유지된다.

따라서 객체 생성 빈도와 생존 패턴은  
GC 빈도와 pause time에 직접적인 영향을 미친다.

이후 장에서 특정 JVM 옵션이 성능에 영향을 준 이유는  
대부분 이 **객체 생명주기 분포 변화**로 설명할 수 있다.

---

### 4.3 GC와 Stop-The-World(STW)

GC는 더 이상 사용되지 않는 객체를 회수하기 위한 메커니즘이다.  
문제는 GC의 일부 단계에서 애플리케이션 스레드가 일시 중지(STW)된다는 점이다.

STW는 다음과 같은 형태로 성능에 영향을 준다.

- 주문 처리 지연 발생
- TPS 순간 하락
- 지연시간 분산 증가 (tail latency 악화)

거래소 엔진처럼 지연시간에 민감한 시스템에서는  
평균 성능보다 **최대 지연시간과 변동성**이 더 중요하다.

이 실험에서 GC 튜닝의 목표는  
GC를 “빠르게 만드는 것”이 아니라,  
**GC로 인한 성능 변동을 예측 가능하게 만드는 것**이다.

---

### 4.4 G1GC를 선택한 이유

본 프로젝트에서는 Java 17의 기본 GC인 G1GC를 사용했다.

G1GC는 다음과 같은 특성을 가진다.

- 힙을 Region 단위로 관리
- 전체 힙을 한 번에 수집하지 않고 점진적으로 처리
- pause time을 목표 값에 맞추려는 동작 특성

이는 대용량 힙과 장시간 실행되는 서비스에서  
**일관된 응답 시간**을 제공하기 위한 설계다.

이번 실험에서는

- 기본 GC 설정이 어느 수준의 성능을 제공하는지
- 추가 튜닝이 실제로 의미 있는 개선을 만드는지

를 확인하는 것이 목적이었기 때문에  
G1GC를 기준점으로 삼았다.

---

### 4.5 Non-Heap 메모리: 실험에서 가장 중요한 영역

JVM 메모리는 힙만으로 구성되지 않는다.  
실제로 운영 환경에서 문제를 일으키는 경우는  
Non-Heap 영역인 경우가 많다.

이 실험에서 특히 중요하게 본 영역은 다음과 같다.

- Metaspace: 클래스 메타데이터
- Code Cache: JIT 컴파일된 코드 저장소
- Direct Memory: NIO 및 네이티브 메모리 사용 영역
- Thread Stack: 스레드 수에 비례해 증가하는 메모리

이 영역들은 힙과 독립적으로 증가하며,  
명시적으로 제한하지 않으면  
컨테이너 환경에서 예기치 않은 OOM으로 이어질 수 있다.

이후 장에서 Non-Heap 제한만으로도  
성능과 안정성이 동시에 개선된 이유는  
이 영역들이 JVM 동작에 미치는 영향이  
생각보다 크기 때문이다.

---

### 4.6 JVM 튜닝의 목적 재정의

이 프로젝트에서의 JVM 튜닝 목적은 다음과 같다.

- 최고 성능을 만드는 것
- 특정 옵션을 정답처럼 제시하는 것

이 아니라,

- 성능 변동의 원인을 이해하고
- 통제 가능한 영역과 불가능한 영역을 구분하며
- 예측 가능한 성능 특성을 확보하는 것

이다.

이제 다음 장에서는  
이 배경을 바탕으로 실제 JVM 옵션 변경이  
어떤 결과를 만들었는지를  
실험 순서대로 살펴본다.

---

## 5. 초기 벤치마크 결과 및 문제 식별

### 5.1 Baseline 측정

JVM 튜닝을 적용하기 전,  
기본 설정 상태에서의 성능을 기준선(Baseline)으로 측정했다.

이 단계의 목적은  
“얼마나 빠른가”를 평가하는 것이 아니라,  
**이후 변경 사항을 비교할 기준점을 확보하는 것**이다.

측정 시 사용한 JVM 설정은 다음과 같다.

- Heap: Xms / Xmx 모두 2GB로 고정
- GC: G1GC
- Non-Heap 관련 제한 없음

벤치마크 결과는 주문 수 1,000 기준으로 다음과 같다.

- limitOrderTps: 2,357 TPS (424.176 μs/op)
- marketBuyTps: 1,649 TPS (606.509 μs/op)
- mixedTps: 1,921 TPS (520.454 μs/op)

GC 동작에 대한 관측 결과는 다음과 같았다.

- 총 GC 발생 횟수: 51회
- 평균 pause time: 1.42ms
- 최대 pause time: 7.53ms
- 최소 pause time: 0.91ms

평균 값만 놓고 보면 큰 문제처럼 보이지 않을 수 있다.  
하지만 거래소 체결 엔진의 특성상  
**문제는 평균이 아니라 최악의 경우**에 있다.

---

### 5.2 관측 결과 해석

Baseline 측정 결과에서 주목한 지점은 TPS 자체가 아니다.  
다음 세 가지 신호가 이후 튜닝의 출발점이 되었다.

첫째, 최대 GC pause time이 7.53ms에 도달했다는 점이다.  
단발성 수치이긴 하지만,  
체결 엔진처럼 지연시간에 민감한 시스템에서는  
이 정도의 pause도 체감 성능 저하로 이어질 수 있다.

둘째, Non-Heap 메모리에 대한 명시적인 제한이 없다는 점이다.  
힙 사용량은 안정적으로 보였지만,  
Metaspace, Code Cache, Direct Memory는  
JVM 외부(native 영역)에서 증가할 수 있는 구조였다.

이는 컨테이너 환경에서 메모리 한계에 도달할 경우,  
Heap OOM이 아닌 형태의 비정상 종료로 이어질 가능성을 내포한다.

셋째, GC pause time의 변동폭이 컸다는 점이다.  
평균 pause time과 최대 pause time 사이의 차이는  
성능이 일정하지 않다는 신호로 해석했다.

거래소 엔진에서 중요한 것은  
“대부분 빠르다”가 아니라  
**“항상 예측 가능하다”는 점**이다.

---

### 5.3 문제 정의

Baseline 측정을 통해 다음과 같이 문제를 정의했다.

- TPS 자체는 러스트 기반 구현과 직접 비교할 대상이 아니며,
  이 시점에서 즉시 개선이 필요한 지표는 아니라고 판단했다.
- 하지만 GC로 인한 지연시간 변동성이 존재한다.
- Non-Heap 메모리는 통제되지 않은 상태다.
- 현재 설정은 성능보다는 **안정성과 예측 가능성 측면에서 위험 요소**를 가진다.

따라서 다음 단계의 목표는  
성능을 극적으로 끌어올리는 것이 아니라,

- GC로 인한 지연을 줄이거나
- 최소한 지연 패턴을 예측 가능하게 만들고
- 운영 환경에서의 메모리 리스크를 제거하는 것

으로 설정했다.

이 문제 정의를 바탕으로  
다음 장부터 단계적인 JVM 튜닝을 시작한다.

---

## 6. JVM 튜닝 과정: 의사결정과 결과

이 장에서는 실제로 JVM 튜닝을 진행하며  
각 단계에서 어떤 문제를 인식했고,  
어떤 가설을 세웠으며,  
측정 결과를 바탕으로 어떤 결정을 내렸는지를 정리한다.

여기서의 목표는  
특정 옵션을 정답처럼 제시하는 것이 아니라,  
**의사결정이 가능했던 이유와 근거를 남기는 것**이다.

---

### 6.1 의사결정 프로세스

모든 튜닝 과정은 다음과 같은 공통된 흐름을 따른다.

문제 인식 → 가설 수립 → 옵션 평가 → 실행 → 측정 → 분석 → 결정

이 프로세스는 성능을 극대화하기 위한 절차가 아니라,  
각 변경이 성능에 미치는 영향을  
**해석 가능하게 만들기 위한 기준**으로 사용했다.

---

### 6.2 Non-Heap 메모리 제한 추가

#### 6.2.1 문제 인식

Baseline 측정 결과, 힙 사용량 자체는 안정적으로 보였지만  
Non-Heap 영역에 대해서는 명시적인 통제가 없는 상태였다.

관찰된 사실은 다음과 같다.

- Metaspace, Code Cache, Direct Memory는 힙과 독립적으로 증가한다
- 현재 설정에서는 전체 JVM 메모리 사용량을 예측하기 어렵다
- 컨테이너 환경에서는 Heap OOM이 아닌 형태로 프로세스가 종료될 수 있다

문제는 단순한 메모리 부족이 아니라,  
**메모리 사용 패턴이 통제되지 않는 상태**라는 점이었다.

---

#### 6.2.2 가설 수립

다음과 같은 가설을 세웠다.

Non-Heap 메모리에 상한을 두면  
메모리 사용량이 예측 가능해지고,  
GC 동작 역시 보다 안정적인 패턴을 보일 수 있다.

예상한 효과는 다음과 같다.

- 메모리 사용량의 예측 가능성 향상
- 운영 환경에서의 OOM 리스크 감소
- GC pause time의 변동폭 감소
- TPS는 유사하거나 소폭 변화

---

#### 6.2.3 옵션 평가

다음과 같은 Non-Heap 관련 옵션을 검토했다.

- MaxMetaspaceSize: 클래스 메타데이터 영역 상한
- ReservedCodeCacheSize: JIT 컴파일 코드 캐시 상한
- MaxDirectMemorySize: NIO 및 네이티브 메모리 사용 제한
- Thread Stack Size: 스레드당 스택 메모리 크기

각 값은 극단적인 최적화가 아니라,  
일반적인 서버 환경에서 무리 없이 사용 가능한 수준으로 설정했다.

---

#### 6.2.4 실행

Non-Heap 메모리 제한을 추가한 상태로  
기존과 동일한 벤치마크를 다시 수행했다.

---

#### 6.2.5 측정 결과

주문 수 1,000 기준 벤치마크 결과는 다음과 같았다.

- limitOrderTps: 2,357 → 2,572 (+9.1%)
- marketBuyTps: 1,649 → 1,843 (+11.8%)
- mixedTps: 1,921 → 2,034 (+5.9%)

GC 관측 지표는 다음과 같이 변화했다.

- 평균 pause time: 1.42ms → 1.29ms
- 최대 pause time: 7.53ms → 2.54ms
- 최소 pause time: 변화 없음

특히 최대 pause time이 크게 감소하면서  
꼬리 지연(tail latency)이 눈에 띄게 개선되었다.

---

#### 6.2.6 분석

이 결과는 예상보다 긍정적이었다.

Non-Heap 제한은 성능을 희생시키는 조치라고 생각하기 쉬우나,  
실제로는 JVM이 사용할 수 있는 메모리 범위를 명확히 인지하면서  
GC 동작이 더 안정적인 패턴을 보인 것으로 해석했다.

또한 Code Cache 제한을 통해  
JIT 컴파일이 불필요하게 확장되는 상황을 방지한 점도  
간접적인 성능 개선 요인으로 보았다.

---

#### 6.2.7 결정

Non-Heap 메모리 제한은 유지하기로 결정했다.

- TPS가 오히려 개선되었고
- 최대 GC pause time이 크게 감소했으며
- 운영 환경에서의 메모리 안정성도 함께 확보할 수 있었다

이 시점에서 가장 중요한 교훈은,  
**메모리 통제가 반드시 성능 저하로 이어지지는 않는다**는 점이었다.

---

### 6.3 GC 변경 시도: G1GC → ZGC

#### 6.3.1 문제 인식

Non-Heap 제한을 통해 최대 pause time은 크게 줄었지만,  
여전히 GC로 인한 지연이 완전히 사라진 것은 아니었다.

거래소 체결 엔진 특성상  
꼬리 지연을 더 줄일 수 있는 선택지가 있는지 검토할 필요가 있었고,  
이 과정에서 GC 자체를 변경하는 방안을 고려하게 되었다.

---

#### 6.3.2 가설 수립

ZGC는 초저지연을 목표로 설계된 GC로,  
pause time을 1ms 이하로 유지하는 것을 주요 목표로 한다.

다음과 같은 가설을 세웠다.

ZGC를 사용하면  
GC pause로 인한 꼬리 지연을 거의 제거할 수 있고,  
성능 변동성 또한 크게 줄어들 것이다.

다만 throughput 손실은 일정 수준 감수해야 할 수 있다고 예상했다.

---

#### 6.3.3 실행 및 측정

ZGC로 GC를 변경한 뒤 동일한 조건에서 벤치마크를 수행했다.

측정 결과는 다음과 같았다.

- 평균 pause time과 최대 pause time은 사실상 0에 수렴
- GC 빈도는 오히려 증가
- 일부 시나리오에서 TPS가 약 9~10% 감소

---

#### 6.3.4 분석

ZGC는 기대한 대로 pause time을 극단적으로 줄여주었다.  
하지만 그 대가로 concurrent GC 작업이 늘어나면서  
CPU 자원을 지속적으로 소모했다.

현재 워크로드는 힙 크기가 2GB로 비교적 작고,
짧은 생명주기의 객체가 빈번하게 생성·소멸되는 특성을 가진다.
이 조건에서는 ZGC의 초저지연 특성이
throughput 손실을 상쇄할 만큼의 이점을 제공하지 못했다.

---

#### 6.3.5 결정

ZGC는 현재 조건에서는 적합하지 않다고 판단하고  
G1GC로 롤백했다.

ZGC는 훌륭한 GC이지만,  
모든 워크로드와 힙 크기에 보편적으로 적합한 선택지는 아니었다.

---

### 6.4 G1GC 파라미터 조정 실험과 기본값의 재확인

ZGC가 적합하지 않다는 결론 이후,  
G1GC를 유지한 상태에서 일부 파라미터 조정을 시도했다.

- MaxGCPauseMillis 조정
- InitiatingHeapOccupancyPercent 조정

두 실험 모두 pause time에는 일부 변화가 있었으나,  
TPS 측면에서는 오히려 불리한 결과를 보였다.

이를 통해 현재 워크로드에서는  
G1GC의 기본 파라미터가 이미 가장 균형 잡힌 선택임을 확인했다.

---

### 6.5 최종 JVM 설정 결정

최종적으로 선택한 JVM 설정은 다음과 같다.

- G1GC 유지
- Heap 크기 고정
- Non-Heap 메모리 제한 적용
- GC 파라미터는 기본값 유지

이 설정은 최고 성능을 목표로 한 선택이 아니라,  
**지연시간 변동이 적고, 예측 가능한 성능 특성을 제공하는 조합**이었다.

이후 장에서는  
GC 압력을 줄이기 위해 수행한 코드 레벨 최적화를 살펴본다.

---

## 7. 코드 최적화 과정: 의사결정과 결과

JVM 튜닝을 통해 GC로 인한 지연 변동성을 일정 수준 통제한 이후,  
코드 레벨에서도 추가적인 성능 개선 여지가 있는지 검토했다.

이 단계의 목적은 절대적인 TPS를 극단적으로 끌어올리는 것이 아니라,  
불필요한 객체 생성과 반복 연산을 제거해 **GC 압력을 완화하고 실행 경로를 단순화**하는 데 있었다.

---

### 7.1 최적화 전략 수립

코드 최적화는 다음과 같은 기준으로 접근했다.

1. 프로파일링을 통해 병목 지점 후보를 식별한다
2. 반복적인 객체 생성이나 계산이 있는지 확인한다
3. 최소한의 코드 변경으로 개선 가능한지를 판단한다
4. 변경 후 동일 조건에서 재측정하여 효과를 검증한다

모든 최적화는 “측정 가능한 변화가 있는가”를 기준으로  
적용 여부를 결정했다.

---

### 7.2 벤치마크 코드 최적화

#### 7.2.1 문제 인식

벤치마크 코드를 분석하던 중,  
주문 생성 루프 내부에서 `BigDecimal` 객체가 매 반복마다 새로 생성되고 있다는 점을 확인했다.

최적화 전 코드는 다음과 같은 형태였다.

```java
@Benchmark
public void limitOrderTps(Blackhole bh) {
    for (int idx = 0; idx < orderCount; idx++) {
        BigDecimal price = BASE_PRICE.add(
            PRICE_STEP.multiply(new BigDecimal(idx % 50))
        );
        BigDecimal amount = new BigDecimal("1.0");
        // ...
    }
}
```

이 구현에는 다음과 같은 문제가 있었다.

- 동일한 가격 패턴이 반복되는데도 매번 새 객체를 생성함
- `BigDecimal` 연산 및 객체 생성 비용이 누적됨
- 불필요한 힙 할당으로 GC 압력을 증가시킴

---

#### 7.2.2 가설 수립

다음과 같은 가설을 세웠다.

> 반복적으로 사용되는 가격 값을 사전에 계산해 테이블로 캐싱하면,  
> 객체 생성 비용을 줄이고 GC 압력을 완화할 수 있을 것이다.

예상 효과는 다음과 같았다.

- 객체 생성 횟수 감소
- Young GC 빈도 감소
- TPS 기준 10~20% 수준의 성능 개선 가능성

---

#### 7.2.3 실행

가격 테이블을 사전에 생성하고,  
루프 내부에서는 이를 참조하도록 코드를 수정했다.

```java
private static final BigDecimal[] LIMIT_PRICE_TABLE =
    buildPriceTable(50, BASE_PRICE, PRICE_STEP);

@Benchmark
public void limitOrderTps(Blackhole bh) {
    for (int idx = 0; idx < orderCount; idx++) {
        BigDecimal price =
            LIMIT_PRICE_TABLE[idx % LIMIT_PRICE_TABLE.length];
        BigDecimal amount = ORDER_AMOUNT;
        // ...
    }
}

```

변경 범위는 벤치마크 코드에 한정했으며,  
현재는 실제 매칭 로직이나 도메인 동작에는 영향을 주지 않도록 했다.

---

#### 7.2.4 측정

JVM 튜닝이 완료된 상태에서 동일한 조건으로 다시 벤치마크를 수행했다  
(주문 수 1,000 기준).

| 벤치마크      | 최적화 전 | 최적화 후 | 변화율 |
| ------------- | --------- | --------- | ------ |
| limitOrderTps | 2,572     | 2,778     | +8.0%  |
| marketBuyTps  | 1,843     | 1,933     | +4.9%  |
| mixedTps      | 2,034     | 2,137     | +5.1%  |

GC 관련 지표는 다음과 같았다.

| 지표            | 최적화 전 | 최적화 후 |
| --------------- | --------- | --------- |
| GC 발생 횟수    | 51회      | 49회      |
| 평균 pause time | 1.29ms    | 1.30ms    |

---

#### 7.2.5 분석

TPS는 예상했던 10~20%에는 미치지 못했지만,  
약 8% 수준의 개선은 명확하게 관측되었다.

예상보다 효과가 제한적이었던 이유는 다음과 같이 해석했다.

1. **웜업의 영향**  
   웜업이 충분하지 않은 상태에서는 JIT 최적화가 완료되지 않아  
   초기 측정에서는 차이가 거의 나타나지 않았다.  
   웜업을 3회로 늘린 이후에야 성능 차이가 안정적으로 관측되었다.

2. **매칭 로직 자체가 주요 병목**  
   객체 생성 비용보다 `TreeMap` 기반 매칭 로직과  
   `BigDecimal` 연산 자체가 더 큰 비중을 차지하고 있었다.

3. **JIT 컴파일러의 선행 최적화**  
   상수 폴딩, 인라이닝 등의 최적화로  
   일부 연산은 이미 JVM 차원에서 비용이 상쇄되고 있었다.

---

#### 7.2.6 결정

BigDecimal 캐싱 최적화는 최종적으로 적용하기로 결정했다.

- TPS가 명확하게 개선되었고
- 코드 변경 범위가 제한적이며
- GC 압력을 완화하는 방향성과도 일치했기 때문이다

이번 최적화를 통해 다음과 같은 점을 다시 확인했다.

- 웜업이 충분하지 않으면 성능 측정은 쉽게 왜곡된다
- 작은 최적화라도 누적되면 의미 있는 개선으로 이어질 수 있다
- 전체 성능을 좌우하는 병목은 알고리즘과 자료구조에 있는 경우가 많다

---

### 7.3 BigDecimal 선택에 대한 회고: 정확성과 성능 사이의 트레이드오프

거래소 엔진 구현 초기에는 가격과 수량 표현을 위해  
`BigDecimal`을 사용하는 것을 자연스럽게 선택했다.

금융 도메인에서는 소수점 오차가 곧 신뢰도 문제로 이어질 수 있고,  
정확성이 최우선이라는 판단이었기 때문이다.  
특히 러스트 구현에서도 고정소수점 기반 연산을 사용했기 때문에,  
자바 마이그레이션 과정에서도 의미적으로 가장 대응되는 타입이 `BigDecimal`이라고 판단했다.

다만 JVM 환경에서 실제 벤치마크와 튜닝을 진행하면서,  
`BigDecimal`이 가지는 비용을 보다 명확히 인식하게 되었다.

- 객체 생성 비용이 크고
- 불변 객체 특성상 연산마다 새로운 객체가 생성되며
- 연산 경로가 길어 JIT 최적화에도 한계가 있다는 점이
  고빈도 매칭 루프에서는 누적 비용으로 작용했다.

이번 최적화에서는 타입 자체를 변경하지는 않았지만,  
가격 테이블 캐싱을 통해 객체 생성 빈도를 줄이는 방식으로  
`BigDecimal`이 가진 비용을 부분적으로 완화했다.

이 경험을 통해 얻은 교훈은 명확하다.

- 초기 설계 단계에서는 **정확성을 우선하는 선택이 합리적**이었고
- 성능 요구가 구체화된 이후에는 **정확성과 성능의 트레이드오프를 다시 평가해야 한다**는 점이다.

만약 실제 운영 환경에서 더 높은 처리량이나 더 낮은 지연 시간이 요구된다면,  
고정소수점(long 기반) 표현이나 도메인 전용 숫자 타입으로의 전환도  
충분히 고려 대상이 될 수 있다.

중요한 것은 특정 선택이 “옳았는가/틀렸는가”가 아니라,  
**요구사항의 변화에 따라 선택을 재검토할 수 있는가**라고 생각한다.

---

## 8. 최종 결과 및 종합 분석

### 8.1 전체 개선 효과

주문 수 1,000 기준으로 각 단계별 성능을 비교한 결과는 다음과 같다.

| 시나리오      | 설정                 | limitOrderTps | 최대 pause time |
| ------------- | -------------------- | ------------- | --------------- |
| 1. Baseline   | JVM 튜닝 없음        | 2,357 TPS     | 7.53ms          |
| 2. JVM 튜닝   | Non-Heap 제한        | 2,572 TPS     | 2.54ms          |
| 3. JVM + 코드 | Non-Heap 제한 + 캐싱 | 2,778 TPS     | 2.50ms          |

Baseline 대비 최종 결과는 다음과 같다.

- TPS는 약 **17.9% 향상**
- 최대 GC pause time은 **약 67% 감소**
- GC pause 분포가 안정화되며 꼬리지연(latency tail)이 크게 완화됨

TPS 자체는 절대적인 수치 개선보다도,  
**지연 시간의 예측 가능성이 확보되었다는 점**이 가장 중요한 변화였다.

---

### 8.2 핵심 인사이트

#### JVM 튜닝 관점

- Non-Heap 메모리 제한은 단순한 안정성 옵션이 아니라  
  JVM이 메모리를 보다 보수적이고 예측 가능하게 사용하도록 만드는 장치였다.
- ZGC는 매우 인상적인 pause time을 제공했지만,  
  현재 워크로드(2GB 힙, 객체 생성 빈도가 높은 구조)에서는  
  throughput 손실이 명확하게 드러났다.
- G1GC의 기본 파라미터는 이미 충분히 성숙해 있으며,  
  명확한 근거 없이 조정하는 것은 오히려 성능을 악화시킬 수 있었다.

#### 코드 최적화 관점

- 객체 생성 제거는 단독으로 큰 효과를 내기보다는  
  GC 압력을 줄이는 방향으로 누적 효과를 만든다.
- 실제 병목은 객체 생성보다 매칭 알고리즘과 자료구조에 있었으며,  
  이는 JVM 튜닝만으로 해결할 수 있는 영역이 아니었다.
- 웜업이 충분하지 않으면 최적화의 효과를 정확히 판단할 수 없다.

#### 의사결정 관점

- 모든 튜닝은 가설 → 실행 → 측정 → 검증의 반복이었다.
- 예상과 다른 결과(ZGC)는 실패가 아니라 판단 기준을 명확히 해주는 실험이었다.
- “왜 안 되는지 설명할 수 있는 상태”가 곧 다음 선택의 출발점이었다.

---

### 8.3 이 프로젝트를 통해 얻은 교훈

- 간단한 설정 변경이 큰 안정성 개선으로 이어질 수 있다.
- 최신 기술이 항상 현재 문제의 해답은 아니다.
- 성능 튜닝에서 가장 위험한 것은 측정 없이 내리는 확신이다.
- latency, throughput, 안정성은 항상 트레이드오프 관계에 있다.

---

## 9. 결론

### 9.1 프로젝트 정리

이번 프로젝트는 단순히 자바 성능을 끌어올리는 실험이 아니었다.

- JVM이 어떤 상황에서 어떤 선택을 하는지 이해했고
- GC 튜닝이 “성능 향상”이 아니라 “성능의 성격을 바꾸는 일”임을 체감했으며
- 실제 수치와 실패 경험을 통해 판단 기준을 갱신했다.

### 9.2 마무리

어떤 언어로 개발하느냐도 중요할 수 있지만 내가 느끼는 백엔드 개발자의 덕목은 **시스템이 언제 느려지고 왜 느려지는지를 설명할 수 있어야 한다**라고 생각한다.

이번 마이그레이션과 튜닝 과정은  
JVM이라는 런타임을 블랙박스가 아닌 **이해 가능한 시스템**으로 바라보게 만든 경험이었다.

---

### 개인적인 회고

처음에 JVM 튜닝을 시작할 때는 솔직히 부담이 컸다.  
주요 경력이 TypeScript 기반이었고, 다른 프로젝트는 Rust로 작성되어 있었기 때문에  
JVM은 나에게 익숙한 영역이 아니었다.

하지만 실제로 튜닝을 진행하면서 느낀 점은 의외로 단순했다.  
접근 방식은 이미 알고 있던 문제 해결 과정과 크게 다르지 않았다.  
메모리 사용을 관찰하고, 지연이 발생하는 지점을 확인하고,  
가설을 세운 뒤 측정으로 검증하는 과정은 언어와 무관한 일이었다.

이번 경험을 통해 얻은 가장 큰 교훈은  
“JVM 튜닝을 할 수 있게 되었다”기보다는,  
**어떤 런타임에서도 동일한 사고 방식으로 문제를 풀 수 있다**는 확신이었다.
